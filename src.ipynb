{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "distributed = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    distributed = torch.cuda.device_count() > 1\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch, Data\n",
    "from torch_geometric.nn import GCNConv, GraphNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondaryStructureDataset(Dataset):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVXWY\"\n",
    "    dssp_types = \"GHITEBSP-\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_one_hot(seq: str, charset: str) -> torch.Tensor:\n",
    "        one_hot = torch.zeros(len(seq), len(charset) + 1, dtype=torch.float)\n",
    "        for i, char in enumerate(seq):\n",
    "            if char in charset:\n",
    "                one_hot[i, charset.index(char) + 1] = 1.0\n",
    "        return one_hot\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_index(seq: str, charset: str) -> torch.Tensor:\n",
    "        indices = torch.zeros(len(seq), dtype=torch.long)\n",
    "        for i, char in enumerate(seq):\n",
    "            if char in charset:\n",
    "                indices[i] = charset.index(char) + 1\n",
    "        return indices\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_label_indices(seq: str, charset: str) -> torch.Tensor:\n",
    "        labels = torch.zeros(len(seq), dtype=torch.long)\n",
    "        for i, char in enumerate(seq):\n",
    "            if char in charset:\n",
    "                labels[i] = charset.index(char)\n",
    "        return labels\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        # Pad x; flatten y to valid positions; return lengths and mask of valid tokens\n",
    "        xs, ys = zip(*batch)\n",
    "        xs_padded = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n",
    "        lengths = torch.tensor([len(x) for x in xs], dtype=torch.long)\n",
    "        mask = (xs_padded != 0)\n",
    "        y_flat = torch.cat([y[:l] for y, l in zip(ys, lengths)], dim=0)\n",
    "        return xs_padded, y_flat, lengths, mask\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,\n",
    "                 seq_col: str = \"sequence\",\n",
    "                 ss_col: str = \"secondary_structure\"):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.seq_col = seq_col\n",
    "        self.ss_col = ss_col\n",
    "        self.max_seq_length = df[seq_col].str.len().max()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        sequence = row[self.seq_col]\n",
    "        secondary_structure = row[self.ss_col]\n",
    "\n",
    "        x = self._to_index(sequence, self.amino_acids)\n",
    "        y = self._to_label_indices(secondary_structure, self.dssp_types)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def class_weights(self):\n",
    "        ss_cat = \"\".join(self.df[self.ss_col])\n",
    "        counts = {char: ss_cat.count(char) for char in self.dssp_types}\n",
    "        total = sum(counts.values())\n",
    "        weights = {char: total / count if count > 0 else 0 for char, count in counts.items()}\n",
    "        return torch.tensor([weights[char] for char in self.dssp_types], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametric Exponential Linear Unit (PELU) activation function\n",
    "class PELU(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(PELU, self).__init__()\n",
    "        self.log_alpha = nn.Parameter(torch.log(torch.tensor(alpha)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        alpha = torch.exp(self.log_alpha)\n",
    "        return torch.where(x >= 0, x, alpha * (torch.exp(x) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = len(SecondaryStructureDataset.amino_acids),\n",
    "                 out_channels: int = len(SecondaryStructureDataset.dssp_types),\n",
    "                 hidden_channels: int = 64,\n",
    "                 num_layers: int = 3):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        self.norms.append(GraphNorm(hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.norms.append(GraphNorm(hidden_channels))\n",
    "\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
    "        self.act = PELU()\n",
    "\n",
    "    def forward(self, data: Data) -> torch.Tensor:\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        for i, (conv, norm) in enumerate(zip(self.convs[:-1], self.norms)):\n",
    "            identity = x\n",
    "            x = conv(x, edge_index)\n",
    "            x = norm(x)\n",
    "            x = self.act(x)\n",
    "            if i != 0:\n",
    "              x = x + identity\n",
    "\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphBuilder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 local_dist: int = 0,\n",
    "                 top_k: int = 32):\n",
    "        super(GraphBuilder, self).__init__()\n",
    "\n",
    "        self.local_dist = local_dist\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor, mask: torch.Tensor) -> Batch:\n",
    "        # x: (N, L, E) embedded features\n",
    "        # lengths: (N,) true sequence lengths\n",
    "        # mask: (N, L) valid positions\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "\n",
    "        # Pairwise similarity scores per batch\n",
    "        scores = torch.matmul(x, x.transpose(1, 2))  # (N, L, L)\n",
    "\n",
    "        # Ignore self and local neighborhood within |i-j| <= local_dist\n",
    "        idx = torch.arange(seq_length, device=x.device)\n",
    "        dist = (idx.unsqueeze(0) - idx.unsqueeze(1)).abs()\n",
    "        scores = scores.masked_fill(dist.unsqueeze(0) <= self.local_dist, float('-inf'))\n",
    "\n",
    "        # Mask padded positions\n",
    "        pad_mask = ~mask  # padded=True\n",
    "        scores = scores.masked_fill(pad_mask.unsqueeze(1), float('-inf'))\n",
    "        scores = scores.masked_fill(pad_mask.unsqueeze(2), float('-inf'))\n",
    "\n",
    "        data_list = []\n",
    "        for b in range(batch_size):\n",
    "            valid_len = int(lengths[b].item())\n",
    "            if valid_len < 2:\n",
    "                data_list.append(Data(x=x[b, :valid_len, :], edge_index=torch.empty((2, 0), dtype=torch.long, device=x.device), edge_attr=torch.empty((0, 1), device=x.device), num_nodes=valid_len))\n",
    "                continue\n",
    "\n",
    "            scores_b = scores[b, :valid_len, :valid_len]\n",
    "            k = min(self.top_k, max(valid_len - 1, 1))\n",
    "\n",
    "            # Top-k neighbor indices per node\n",
    "            topk_vals, topk_idx = torch.topk(scores_b, k=k, dim=-1)  # (valid_len, k)\n",
    "\n",
    "            src = torch.arange(valid_len, device=x.device).unsqueeze(1).expand(valid_len, k)\n",
    "            flat_src = src.reshape(-1)\n",
    "            flat_dst = topk_idx.reshape(-1)\n",
    "            flat_scores = topk_vals.reshape(-1)\n",
    "            keep = torch.isfinite(flat_scores)\n",
    "            flat_src = flat_src[keep]\n",
    "            flat_dst = flat_dst[keep]\n",
    "\n",
    "            if flat_src.numel() == 0:\n",
    "                data_list.append(Data(x=x[b, :valid_len, :], edge_index=torch.empty((2, 0), dtype=torch.long, device=x.device), edge_attr=torch.empty((0, 1), device=x.device), num_nodes=valid_len))\n",
    "                continue\n",
    "\n",
    "            directed_edges = torch.stack([flat_src, flat_dst], dim=0)  # (2, M)\n",
    "\n",
    "            # Build all bidirectional edges first, then deduplicate directed edges\n",
    "            bidir_initial = torch.cat([directed_edges, directed_edges.flip(0)], dim=1)  # (2, 2M)\n",
    "            bidir_edges, _ = torch.unique(bidir_initial, dim=1, return_inverse=True)\n",
    "\n",
    "            # Edge scores per directed edge\n",
    "            edge_scores = scores_b[bidir_edges[0], bidir_edges[1]].unsqueeze(1)\n",
    "\n",
    "            data_list.append(Data(\n",
    "                x=x[b, :valid_len, :],\n",
    "                edge_index=bidir_edges,\n",
    "                edge_attr=edge_scores,\n",
    "                num_nodes=valid_len,\n",
    "                edge_score=edge_scores\n",
    "            ))\n",
    "\n",
    "        return Batch.from_data_list(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 amino_acid_dim: int = len(SecondaryStructureDataset.amino_acids),\n",
    "                 max_length: int = 2048,\n",
    "                 embed_dim: int = 32,\n",
    "                 kernel_size: int = 7,\n",
    "                 num_layers: int = 3):\n",
    "        super(Embedder, self).__init__()\n",
    "\n",
    "        # amino_acid_dim is the count of characters; indices run 0..amino_acid_dim\n",
    "        # so embedding needs +1 to accommodate padding (0) and max index (amino_acid_dim)\n",
    "        self.amino_acid_dim = amino_acid_dim\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(amino_acid_dim + 1, embed_dim, padding_idx=0)\n",
    "\n",
    "        # Sinusoidal positional encoding\n",
    "        self.register_buffer(\"positional_encoding\", self._gen_positional_encoding(max_length))\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.conv_layers.extend([\n",
    "                nn.Conv1d(embed_dim, embed_dim, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "                nn.GELU(),\n",
    "            ])\n",
    "\n",
    "    def _gen_positional_encoding(self, length: int) -> torch.Tensor:\n",
    "        pe = torch.zeros(length, self.embed_dim)\n",
    "        position = torch.arange(0, length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / self.embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (N, L) LongTensor of indices\n",
    "        # lengths: (N,) LongTensor of sequence lengths\n",
    "        # mask: (N, L) BoolTensor where True indicates valid positions\n",
    "        batch_size, seq_length = x.size()\n",
    "\n",
    "        x = self.embedding(x)  # (N, L, E)\n",
    "\n",
    "        # Ensure positional encoding on same device and dtype\n",
    "        pos = self.positional_encoding[:seq_length, :].unsqueeze(0).to(x.device, dtype=x.dtype)\n",
    "        x = x + pos  # Add positional encoding\n",
    "\n",
    "        # Apply mask to zero out embeddings at padding positions\n",
    "        x = x * mask.unsqueeze(2).float()\n",
    "\n",
    "        x = x.transpose(1, 2)  # (N, E, L)\n",
    "\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = x.transpose(1, 2)  # (N, L, E)\n",
    "\n",
    "        # Apply mask again\n",
    "        x = x * mask.unsqueeze(2).float()\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondaryStructurePredictor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int = 32,\n",
    "                 embed_kernel_size: int = 7,\n",
    "                 embed_num_layers: int = 3,\n",
    "                 embed_max_length: int = 2048,\n",
    "                 embed_amino_acid_dim: int = len(SecondaryStructureDataset.amino_acids),\n",
    "                 gcn_out_channels: int = len(SecondaryStructureDataset.dssp_types),\n",
    "                 gcn_hidden_channels: int = 64,\n",
    "                 gcn_num_layers: int = 3,\n",
    "                 edge_local_dist: int = 0,\n",
    "                 edge_top_k: int = 32):\n",
    "        super(SecondaryStructurePredictor, self).__init__()\n",
    "        self.embedder = Embedder(\n",
    "            amino_acid_dim=embed_amino_acid_dim,\n",
    "            max_length=embed_max_length,\n",
    "            embed_dim=embed_dim,\n",
    "            kernel_size=embed_kernel_size,\n",
    "            num_layers=embed_num_layers\n",
    "        )\n",
    "        self.edge_model = GraphBuilder(\n",
    "            local_dist=edge_local_dist,\n",
    "            top_k=edge_top_k\n",
    "        )\n",
    "        self.gcn_model = GCN(\n",
    "            in_channels=embed_dim,\n",
    "            out_channels=gcn_out_channels,\n",
    "            hidden_channels=gcn_hidden_channels,\n",
    "            num_layers=gcn_num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedder(x, lengths, mask)  # (N, L, E)\n",
    "        graph_data = self.edge_model(x, lengths, mask)\n",
    "        out = self.gcn_model(graph_data)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics import MetricCollection, Accuracy, F1Score, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch: int,\n",
    "          model: SecondaryStructurePredictor,\n",
    "          loader: DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "          criterion: nn.Module,\n",
    "          device: str | torch.device | int,\n",
    "          batch_print_freq: int = 32,\n",
    "          writer: SummaryWriter = None,\n",
    "          distributed: bool = False) -> float:\n",
    "    model.train()\n",
    "\n",
    "    total_loss = torch.zeros(1, device=device, requires_grad=False)\n",
    "    running_loss = torch.zeros(1, device=device, requires_grad=False)\n",
    "    total_count = torch.zeros(1, device=device, requires_grad=False)\n",
    "    running_count = torch.zeros(1, device=device, requires_grad=False)\n",
    "\n",
    "    num_class = len(SecondaryStructureDataset.dssp_types)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    for avg in [None, 'macro', 'micro', 'weighted']:\n",
    "        metrics[f'accuracy_{avg}'] = Accuracy(num_classes=num_class, task='multiclass', average=avg)\n",
    "        metrics[f'f1_score_{avg}'] = F1Score(num_classes=num_class, task='multiclass', average=avg)\n",
    "        metrics[f'precision_{avg}'] = Precision(num_classes=num_class, task='multiclass', average=avg)\n",
    "        metrics[f'recall_{avg}'] = Recall(num_classes=num_class, task='multiclass', average=avg)\n",
    "\n",
    "    distTorchMetricsBatch = MetricCollection(metrics=metrics).to(device)\n",
    "\n",
    "    distTorchMetricsEpoch = MetricCollection(metrics=metrics).to(device)\n",
    "\n",
    "    for i, (x, y, lengths, mask) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x, lengths, mask)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        node_count = y.numel()\n",
    "        total_loss += loss.item() * node_count\n",
    "        running_loss += loss.item() * node_count\n",
    "        total_count += node_count\n",
    "        running_count += node_count\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        preds = out.argmax(dim=1)\n",
    "        distTorchMetricsBatch.update(preds, y)\n",
    "        distTorchMetricsEpoch.update(preds, y)\n",
    "\n",
    "        if i % batch_print_freq == 0 and (writer is not None or distributed):\n",
    "            if distributed:\n",
    "                torch.distributed.all_reduce(\n",
    "                    running_loss,\n",
    "                    op=torch.distributed.ReduceOp.SUM\n",
    "                )\n",
    "                torch.distributed.all_reduce(\n",
    "                    running_count,\n",
    "                    op=torch.distributed.ReduceOp.SUM\n",
    "                )\n",
    "\n",
    "            if writer is not None:\n",
    "                local_stats = distTorchMetricsBatch.compute()\n",
    "                for key, value in local_stats.items():\n",
    "                    if distributed:\n",
    "                        torch.distributed.all_reduce(value, op=torch.distributed.ReduceOp.SUM)\n",
    "                        value /= torch.distributed.get_world_size()\n",
    "                    if value.dim() > 0 and value.size(0) > 1:\n",
    "                        for idx, cls in enumerate(SecondaryStructureDataset.dssp_types):\n",
    "                            writer.add_scalar(f\"{key}/train/batch/{cls}\", value[idx].item(), epoch * len(loader) + i)\n",
    "                    else:\n",
    "                        writer.add_scalar(f\"{key}/train/batch\", value.item(), epoch * len(loader) + i)\n",
    "\n",
    "                writer.add_scalar(\"Loss/train/batch\", running_loss.item() / running_count.item(), epoch * len(loader) + i)\n",
    "                distTorchMetricsBatch.reset()\n",
    "\n",
    "            running_loss.zero_()\n",
    "            running_count.zero_()\n",
    "\n",
    "    if distributed:\n",
    "        torch.distributed.all_reduce(total_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        torch.distributed.all_reduce(total_count, op=torch.distributed.ReduceOp.SUM)\n",
    "    total_loss /= total_count.item()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_scalar(\"Loss/train/epoch\", total_loss.item(), epoch)\n",
    "\n",
    "    epoch_stats = distTorchMetricsEpoch.compute()\n",
    "\n",
    "    for key, value in epoch_stats.items():\n",
    "        if distributed:\n",
    "            torch.distributed.all_reduce(value, op=torch.distributed.ReduceOp.SUM)\n",
    "            value /= torch.distributed.get_world_size()\n",
    "        if writer is not None:\n",
    "            if value.dim() > 0 and value.size(0) > 1:\n",
    "                for idx, cls in enumerate(SecondaryStructureDataset.dssp_types):\n",
    "                    writer.add_scalar(f\"{key}/train/epoch/{cls}\", value[idx].item(), epoch)\n",
    "            else:\n",
    "                writer.add_scalar(f\"{key}/train/epoch\", value.item(), epoch)\n",
    "\n",
    "    return total_loss.item(), epoch_stats['accuracy_macro'].item(), epoch_stats['f1_score_macro'].item(), epoch_stats['precision_macro'].item(), epoch_stats['recall_macro'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: SecondaryStructurePredictor,\n",
    "            loader: DataLoader,\n",
    "            criterion: nn.Module,\n",
    "            dataset_name: str,\n",
    "            device: str | torch.device | int,\n",
    "            epoch: int = 0,\n",
    "            writer: SummaryWriter = None,\n",
    "            distributed: bool = False) -> float:\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = torch.zeros(1, device=device, requires_grad=False)\n",
    "    total_count = torch.zeros(1, device=device, requires_grad=False)\n",
    "\n",
    "    num_class = len(SecondaryStructureDataset.dssp_types)\n",
    "\n",
    "    metricDict = {}\n",
    "\n",
    "    for avg in [None, 'macro', 'micro', 'weighted']:\n",
    "        metricDict[f'accuracy_{avg}'] = Accuracy(num_classes=num_class, task='multiclass', average=avg)\n",
    "        metricDict[f'f1_score_{avg}'] = F1Score(num_classes=num_class, task='multiclass', average=avg)\n",
    "        metricDict[f'precision_{avg}'] = Precision(num_classes=num_class, task='multiclass', average=avg)\n",
    "        metricDict[f'recall_{avg}'] = Recall(num_classes=num_class, task='multiclass', average=avg)\n",
    "\n",
    "    metrics = MetricCollection(metricDict).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (x, y, lengths, mask) in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            out = model(x, lengths, mask)\n",
    "            loss = criterion(out, y)\n",
    "\n",
    "            node_count = y.numel()\n",
    "            total_loss += loss.item() * node_count\n",
    "            total_count += node_count\n",
    "\n",
    "            metrics.update(out.argmax(dim=1), y)\n",
    "\n",
    "    if distributed:\n",
    "        torch.distributed.all_reduce(total_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        torch.distributed.all_reduce(total_count, op=torch.distributed.ReduceOp.SUM)\n",
    "\n",
    "    total_loss /= total_count.item()\n",
    "\n",
    "    # Compute metrics\n",
    "    stats = metrics.compute()\n",
    "\n",
    "    # Handle distributed metrics\n",
    "    if distributed:\n",
    "        for key, value in stats.items():\n",
    "            torch.distributed.all_reduce(value, op=torch.distributed.ReduceOp.SUM)\n",
    "            value /= torch.distributed.get_world_size()\n",
    "\n",
    "    # Write to tensorboard\n",
    "    if writer is not None:\n",
    "        writer.add_scalar(f\"Loss/{dataset_name}/epoch\", total_loss.item(), epoch)\n",
    "        for key, value in stats.items():\n",
    "            if value.dim() > 0 and value.size(0) > 1:\n",
    "                for idx, cls in enumerate(SecondaryStructureDataset.dssp_types):\n",
    "                    writer.add_scalar(f\"{key}/{dataset_name}/epoch/{cls}\", stats[key][idx].item(), epoch)\n",
    "            else:\n",
    "                writer.add_scalar(f\"{key}/{dataset_name}/epoch\", value.item(), epoch)\n",
    "\n",
    "    return total_loss.item(), stats['f1_score_macro'].item(), stats['recall_macro'].item(), stats['precision_macro'].item(), stats['accuracy_macro'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    torch.cuda.set_device(rank)\n",
    "    init_process_group(\"gloo\", rank=rank, world_size=world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "        save_every: int,\n",
    "        total_epochs: int,\n",
    "        train_dataloader: DataLoader,\n",
    "        val_dataloader: DataLoader,\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str | torch.device | int,\n",
    "        max_seq_length: int,\n",
    "        save_path: str = \"\",\n",
    "        distributed: bool = False):\n",
    "    model = SecondaryStructurePredictor(\n",
    "        embed_dim=32,\n",
    "        embed_kernel_size=7,\n",
    "        embed_num_layers=5,\n",
    "        embed_max_length=max_seq_length,\n",
    "        gcn_out_channels=len(SecondaryStructureDataset.dssp_types),\n",
    "        gcn_hidden_channels=128,\n",
    "        gcn_num_layers=10,\n",
    "        edge_local_dist=0,\n",
    "        edge_top_k=32\n",
    "    ).to(device)\n",
    "    model_name = f\"SecondCount-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "    if distributed:\n",
    "        model = DDP(model, device_ids=[device])\n",
    "\n",
    "    writer = SummaryWriter(os.path.join(save_path, f\"runs/{model_name}\")) if not distributed or torch.distributed.get_rank() == 0 else None\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(weight=train_dataloader.dataset.class_weights().to(device))\n",
    "\n",
    "    steps_per_epoch = len(train_dataloader)\n",
    "\n",
    "    warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer=optimizer,\n",
    "        start_factor=0.1,\n",
    "        total_iters=steps_per_epoch // 5\n",
    "    )\n",
    "\n",
    "    cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer=optimizer,\n",
    "        T_0=steps_per_epoch * 25\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ChainedScheduler(\n",
    "        schedulers=[warmup, cosine]\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    pathlib.Path(os.path.join(save_path, f\"models/{model_name}\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{total_epochs}\")\n",
    "\n",
    "        train_loss, train_accuracy, train_f1, train_precision, train_recall = train(\n",
    "            epoch, model, train_dataloader, optimizer, scheduler, criterion, device,\n",
    "            batch_print_freq=save_every, writer=writer, distributed=distributed\n",
    "        )\n",
    "        if not distributed or torch.distributed.get_rank() == 0:\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, F1: {train_f1:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        val_loss, val_f1, val_recall, val_precision, val_accuracy = evaluate(\n",
    "            model, val_dataloader, criterion, \"val\", device, epoch, writer, distributed,\n",
    "        )\n",
    "        if not distributed or torch.distributed.get_rank() == 0:\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, F1: {val_f1:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(save_path, f\"models/{model_name}/best_model.pth\"))\n",
    "            print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        save_state = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "        }\n",
    "\n",
    "        torch.save(save_state, os.path.join(save_path, f\"models/{model_name}/checkpoint-{epoch}{f\"-{device}\" if distributed else \"\"}.pth\"))\n",
    "\n",
    "    if not distributed or torch.distributed.get_rank() == 0:\n",
    "        print(\"Training complete. Evaluating on test set...\")\n",
    "    test_loss, test_f1, test_recall, test_precision, test_accuracy = evaluate(\n",
    "        model, test_dataloader, criterion, \"test\", device, writer=writer, distributed=distributed\n",
    "    )\n",
    "\n",
    "    if not distributed or torch.distributed.get_rank() == 0:\n",
    "        print(f\"Test Loss: {test_loss:.4f}, F1: {test_f1:.4f}, Recall: {test_recall:.4f}, Precision: {test_precision:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
    "        writer.close()\n",
    "\n",
    "    if distributed:\n",
    "        destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_run(rank: int,\n",
    "             world_size: int,\n",
    "             save_every: int,\n",
    "             total_epochs: int,\n",
    "             batch_size: int,\n",
    "             train_dataset: SecondaryStructureDataset,\n",
    "             val_dataset: SecondaryStructureDataset,\n",
    "             test_dataset: SecondaryStructureDataset,\n",
    "             max_seq_length: int):\n",
    "    ddp_setup(rank, world_size)\n",
    "\n",
    "    torch.manual_seed(432250798121)\n",
    "\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank)\n",
    "    test_sampler = DistributedSampler(test_dataset, num_replicas=world_size, rank=rank)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "    run(\n",
    "        save_every=save_every,\n",
    "        total_epochs=total_epochs,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        test_dataloader=test_loader,\n",
    "        device=rank,\n",
    "        max_seq_length=max_seq_length,\n",
    "        distributed=True,\n",
    "        save_path=\"drive/MyDrive/SecondCount\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_df = pd.read_csv(\"data/pdb/train_data.csv\")\n",
    "    val_df = pd.read_csv(\"data/pdb/val_data.csv\")\n",
    "    test_df = pd.read_csv(\"data/pdb/test_data.csv\")\n",
    "\n",
    "    # Calculate global max_seq_length\n",
    "    all_sequences = pd.concat([train_df[\"sequence\"], val_df[\"sequence\"], test_df[\"sequence\"]])\n",
    "    global_max_seq_length = all_sequences.str.len().max()\n",
    "    print(f\"Global maximum sequence length: {global_max_seq_length}\")\n",
    "\n",
    "    train_dataset = SecondaryStructureDataset(train_df)\n",
    "    val_dataset = SecondaryStructureDataset(val_df)\n",
    "    test_dataset = SecondaryStructureDataset(test_df)\n",
    "\n",
    "    batch_size = 8\n",
    "\n",
    "    # The sampler will handle shuffling if distributed is True\n",
    "    training_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=not distributed, sampler=DistributedSampler(train_dataset) if distributed else None, collate_fn=SecondaryStructureDataset.collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=not distributed, sampler=DistributedSampler(val_dataset) if distributed else None, collate_fn=SecondaryStructureDataset.collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=not distributed, sampler=DistributedSampler(test_dataset) if distributed else None, collate_fn=SecondaryStructureDataset.collate_fn)\n",
    "\n",
    "    if distributed:\n",
    "        world_size = torch.cuda.device_count()\n",
    "        mp.spawn(dist_run, args=(world_size, 32, 10, batch_size, train_dataset, val_dataset, test_dataset, global_max_seq_length), nprocs=world_size, join=True)\n",
    "    else:\n",
    "        run(\n",
    "            save_every=32,\n",
    "            total_epochs=10,\n",
    "            train_dataloader=training_loader,\n",
    "            val_dataloader=val_loader,\n",
    "            test_dataloader=test_loader,\n",
    "            device=device,\n",
    "            max_seq_length=global_max_seq_length, # Pass global_max_seq_length\n",
    "            distributed=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SecondCount",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
