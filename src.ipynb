{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rcsb_data = pd.read_csv(\"data/rcsb/RCSB_PDB_Macromolecular_Structure_Dataset_with_Structural_Features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = rcsb_data.loc[:,[\"Sequence\", \"Number of Residues\", \"Molecular Weight per Deposited Model\", \"Molecular Weight (Entity)\", \"R Free\", \"R Work\", \"Helix\", \"Sheet\", \"Coil\"]]\n",
    "filtered_data = filtered_data.dropna(subset=[\"Helix\", \"Sheet\", \"Coil\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amino_acid_tokenizer(amino_acid : str) -> torch.Tensor:\n",
    "    amino_acid_tokens = {\n",
    "        \"A\": 1,\n",
    "        \"C\": 2,\n",
    "        \"D\": 3,\n",
    "        \"E\": 4,\n",
    "        \"F\": 5,\n",
    "        \"G\": 6,\n",
    "        \"H\": 7,\n",
    "        \"I\": 8,\n",
    "        \"K\": 9,\n",
    "        \"L\": 10,\n",
    "        \"M\": 11,\n",
    "        \"N\": 12,\n",
    "        \"P\": 13,\n",
    "        \"Q\": 14,\n",
    "        \"R\": 15,\n",
    "        \"S\": 16,\n",
    "        \"T\": 17,\n",
    "        \"V\": 18,\n",
    "        \"W\": 19,\n",
    "        \"Y\": 20,\n",
    "        \"X\": 21,\n",
    "        \"U\": 22,\n",
    "        \"O\": 23\n",
    "    }\n",
    "    return torch.tensor([amino_acid_tokens[aa] for aa in amino_acid], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data[\"Sequence\"] = filtered_data[\"Sequence\"].apply(amino_acid_tokenizer)\n",
    "\n",
    "# Padding sequences with zeros to make them all the same length\n",
    "max_sequence_length = filtered_data[\"Sequence\"].apply(len).max()\n",
    "filtered_data[\"Sequence\"] = filtered_data[\"Sequence\"].apply(lambda x: torch.nn.functional.pad(x, (0, max_sequence_length - len(x)), \"constant\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrambled_data = filtered_data.sample(frac=1)\n",
    "scrambled_data = scrambled_data.reset_index(drop=True)\n",
    "\n",
    "data_size = len(scrambled_data)\n",
    "train_size = int(data_size * 0.8)\n",
    "test_size = data_size - train_size\n",
    "validation_size = int(train_size * 0.2)\n",
    "\n",
    "train_data = scrambled_data.iloc[:train_size - validation_size]\n",
    "validation_data = scrambled_data.iloc[train_size - validation_size:train_size]\n",
    "test_data = scrambled_data.iloc[train_size:]\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "validation_data = validation_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_df = train_data.loc[:,[\"Sequence\", \"Number of Residues\", \"Molecular Weight per Deposited Model\", \"Molecular Weight (Entity)\", \"R Free\", \"R Work\"]]\n",
    "train_output_df = train_data.loc[:,[\"Helix\", \"Sheet\", \"Coil\"]]\n",
    "validation_input_df = validation_data.loc[:,[\"Sequence\", \"Number of Residues\", \"Molecular Weight per Deposited Model\", \"Molecular Weight (Entity)\", \"R Free\", \"R Work\"]]\n",
    "validation_output_df = validation_data.loc[:,[\"Helix\", \"Sheet\", \"Coil\"]]\n",
    "test_input_df = test_data.loc[:,[\"Sequence\", \"Number of Residues\", \"Molecular Weight per Deposited Model\", \"Molecular Weight (Entity)\", \"R Free\", \"R Work\"]]\n",
    "test_output_df = test_data.loc[:,[\"Helix\", \"Sheet\", \"Coil\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(input_df, output_df):\n",
    "    input_tensors = torch.tensor(input_df.drop(columns=[\"Sequence\"]).values, dtype=torch.float32)\n",
    "    input_tensor_sequences = torch.stack(tuple(input_df[\"Sequence\"].values), 0)\n",
    "\n",
    "    output_tensors = torch.tensor(output_df.values, dtype=torch.float32)\n",
    "\n",
    "    return torch.utils.data.TensorDataset(input_tensors, input_tensor_sequences, output_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_input_df, train_output_df)\n",
    "validation_dataset = create_dataset(validation_input_df, validation_output_df)\n",
    "test_dataset = create_dataset(test_input_df, test_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch : int,\n",
    "          model : nn.Module,\n",
    "          device: torch.device,\n",
    "          train_dataloader : DataLoader,\n",
    "          optimizer : Optimizer,\n",
    "          loss_fn : nn.Module,\n",
    "          tensorboard_writer : SummaryWriter = None) -> float:\n",
    "    running_loss = 0.\n",
    "\n",
    "    model.train(True)\n",
    "\n",
    "    for idx, (input_tensors, input_tensor_sequences, output_tensors) in enumerate(train_dataloader):\n",
    "        input_tensors = input_tensors.to(device)\n",
    "        input_tensor_sequences = input_tensor_sequences.to(device)\n",
    "        output_tensors = output_tensors.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input_tensors, input_tensor_sequences)\n",
    "        loss = loss_fn(output, output_tensors)\n",
    "  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "  \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if tensorboard_writer is not None:\n",
    "            tensorboard_writer.add_scalar(\"Loss/train\", loss.item(), epoch * len(train_dataloader) + idx)\n",
    "\n",
    "    running_loss /= len(train_dataloader)\n",
    "\n",
    "    if tensorboard_writer is not None:\n",
    "        tensorboard_writer.add_scalar(\"Loss/train/epoch\", running_loss, epoch)\n",
    "    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch : int, \n",
    "             model : nn.Module,\n",
    "             device : torch.device,\n",
    "             validation_dataloader : DataLoader,\n",
    "             loss_fn : nn.Module,\n",
    "             tensorboard_writer : SummaryWriter = None) -> float:\n",
    "    running_loss = 0.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (input_tensors, input_tensor_sequences, output_tensors) in enumerate(validation_dataloader):\n",
    "            input_tensors = input_tensors.to(device)\n",
    "            input_tensor_sequences = input_tensor_sequences.to(device)\n",
    "            output_tensors = output_tensors.to(device)\n",
    "\n",
    "            output = model(input_tensors, input_tensor_sequences)\n",
    "            loss = loss_fn(output, output_tensors)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if tensorboard_writer is not None:\n",
    "                tensorboard_writer.add_scalar(\"Loss/validation\", loss.item(), epoch * len(validation_dataloader) + idx)\n",
    "    \n",
    "    running_loss /= len(validation_dataloader)\n",
    "\n",
    "    if tensorboard_writer is not None:\n",
    "        tensorboard_writer.add_scalar(\"Loss/validation/epoch\", running_loss, epoch)\n",
    "    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model : nn.Module,\n",
    "         device : torch.device,\n",
    "         test_dataloader : DataLoader,\n",
    "         loss_fn : nn.Module) -> float:\n",
    "    running_loss = 0.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (input_tensors, input_tensor_sequences, output_tensors) in enumerate(test_dataloader):\n",
    "            input_tensors = input_tensors.to(device)\n",
    "            input_tensor_sequences = input_tensor_sequences.to(device)\n",
    "            output_tensors = output_tensors.to(device)\n",
    "\n",
    "            output = model(input_tensors, input_tensor_sequences)\n",
    "            loss = loss_fn(output, output_tensors)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    running_loss /= len(test_dataloader)\n",
    "\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model : nn.Module,\n",
    "                device : torch.device,\n",
    "                training_dataloader : DataLoader,\n",
    "                validation_dataloader : DataLoader,\n",
    "                test_dataloader : DataLoader,\n",
    "                optimizer : Optimizer,\n",
    "                loss_fn : nn.Module,\n",
    "                epochs : int,\n",
    "                model_name : str = \"SecondCount\") -> float:\n",
    "    model.to(device)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    tensorboard_writer = SummaryWriter(f\"runs/{model_name}_{timestamp}\")\n",
    "\n",
    "    best_validation_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(epoch, model, device, training_dataloader, optimizer, loss_fn, tensorboard_writer)\n",
    "        validation_loss = validate(epoch, model, device, validation_dataloader, loss_fn, tensorboard_writer)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss} - Validation Loss: {validation_loss}\")\n",
    "\n",
    "        if validation_loss < best_validation_loss:\n",
    "            best_validation_loss = validation_loss\n",
    "            torch.save(model.state_dict(), f\"models/{model_name}_{timestamp}_{epoch}.pt\")\n",
    "\n",
    "\n",
    "    test_loss = test(model, device, test_dataloader, loss_fn)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "    tensorboard_writer.close()\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondCountModel(nn.Module):\n",
    "    def __init__(self, max_sequence_length : int, structural_features : int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.structural_features = structural_features\n",
    "\n",
    "        # Encoder-Decoder LSTM Model\n",
    "\n",
    "        self.encoder_lstm = nn.LSTM(input_size=max_sequence_length, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(input_size=max_sequence_length, hidden_size=128, num_layers=1, batch_first=True)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 + structural_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_tensors, input_tensor_sequences):\n",
    "        # Encoding & decoding the input sequences\n",
    "        encoder_output, encoder_hidden = self.encoder_lstm(input_tensor_sequences)\n",
    "        decoder_output, decoder_hidden = self.decoder_lstm(input_tensor_sequences, encoder_hidden)\n",
    "\n",
    "        # Applying values from input_tensors and encoded_decoded to the fully connected layer\n",
    "        concatenated = torch.cat((input_tensors, decoder_output), 1)\n",
    "        return self.fc(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SecondCountModel(max_sequence_length, train_dataset.tensors[0].shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128 - Train Loss: 59402.222903588976 - Validation Loss: 53202.88383957435\n",
      "Epoch 2/128 - Train Loss: 58865.83254298581 - Validation Loss: 52347.676892510775\n",
      "Epoch 3/128 - Train Loss: 57377.99743279203 - Validation Loss: 50717.086442618536\n",
      "Epoch 4/128 - Train Loss: 55205.195602483625 - Validation Loss: 47894.9819841056\n",
      "Epoch 5/128 - Train Loss: 52619.71752183406 - Validation Loss: 45707.56623787715\n",
      "Epoch 6/128 - Train Loss: 49609.37462046261 - Validation Loss: 42130.80334472656\n",
      "Epoch 7/128 - Train Loss: 46508.13101716021 - Validation Loss: 39363.22994679418\n",
      "Epoch 8/128 - Train Loss: 43460.52789813046 - Validation Loss: 36916.359240301725\n",
      "Epoch 9/128 - Train Loss: 40564.13683389056 - Validation Loss: 33796.24590853987\n",
      "Epoch 10/128 - Train Loss: 37983.433672642605 - Validation Loss: 32029.446987809806\n",
      "Epoch 11/128 - Train Loss: 35749.94964519651 - Validation Loss: 29991.715769800645\n",
      "Epoch 12/128 - Train Loss: 33864.02565502183 - Validation Loss: 27907.6593985722\n",
      "Epoch 13/128 - Train Loss: 32305.739842044215 - Validation Loss: 26617.886508283944\n",
      "Epoch 14/128 - Train Loss: 31084.483225726664 - Validation Loss: 25800.007282125538\n",
      "Epoch 15/128 - Train Loss: 30172.975521970526 - Validation Loss: 25704.752037311424\n",
      "Epoch 16/128 - Train Loss: 29531.73673324918 - Validation Loss: 24326.489375673493\n",
      "Epoch 17/128 - Train Loss: 29116.50388919214 - Validation Loss: 24065.67363449623\n",
      "Epoch 18/128 - Train Loss: 28909.2465457833 - Validation Loss: 23902.327097925645\n",
      "Epoch 19/128 - Train Loss: 28709.488797250273 - Validation Loss: 23873.09713429418\n",
      "Epoch 20/128 - Train Loss: 28659.491349532615 - Validation Loss: 23773.55104222791\n",
      "Epoch 21/128 - Train Loss: 28600.630637622817 - Validation Loss: 23813.38522023168\n",
      "Epoch 22/128 - Train Loss: 28584.386403179586 - Validation Loss: 24040.49831627155\n",
      "Epoch 23/128 - Train Loss: 28579.68861302538 - Validation Loss: 25429.239998653018\n",
      "Epoch 24/128 - Train Loss: 28623.10211048376 - Validation Loss: 24175.783203125\n",
      "Epoch 25/128 - Train Loss: 28583.069451078056 - Validation Loss: 25439.04248046875\n",
      "Epoch 26/128 - Train Loss: 28624.89335212541 - Validation Loss: 23895.55877896013\n",
      "Epoch 27/128 - Train Loss: 28572.894192225027 - Validation Loss: 23838.573865167025\n",
      "Epoch 28/128 - Train Loss: 28620.894927845253 - Validation Loss: 24226.20087722252\n",
      "Epoch 29/128 - Train Loss: 28589.204293463426 - Validation Loss: 23779.747474407326\n",
      "Epoch 30/128 - Train Loss: 28582.971504844434 - Validation Loss: 24163.891231142243\n",
      "Epoch 31/128 - Train Loss: 28587.566041638238 - Validation Loss: 24351.246792497306\n",
      "Epoch 32/128 - Train Loss: 28590.729756584333 - Validation Loss: 23957.082115436424\n",
      "Epoch 33/128 - Train Loss: 28580.946572649427 - Validation Loss: 23851.23605872845\n",
      "Epoch 34/128 - Train Loss: 28577.54310308065 - Validation Loss: 23789.347151131464\n",
      "Epoch 35/128 - Train Loss: 28580.284495257914 - Validation Loss: 23815.189663591056\n",
      "Epoch 36/128 - Train Loss: 28570.797277991947 - Validation Loss: 23724.582351158406\n",
      "Epoch 37/128 - Train Loss: 28594.1013919214 - Validation Loss: 23766.554510708513\n",
      "Epoch 38/128 - Train Loss: 28584.54965116676 - Validation Loss: 23736.827030576507\n",
      "Epoch 39/128 - Train Loss: 28576.76231790734 - Validation Loss: 23888.360898774245\n",
      "Epoch 40/128 - Train Loss: 28594.358696950054 - Validation Loss: 24391.854576373924\n",
      "Epoch 41/128 - Train Loss: 28588.943489441186 - Validation Loss: 23761.661040207437\n",
      "Epoch 42/128 - Train Loss: 28641.910241539303 - Validation Loss: 24182.040577855605\n",
      "Epoch 43/128 - Train Loss: 28581.250006396698 - Validation Loss: 23881.95676185345\n",
      "Epoch 44/128 - Train Loss: 28570.968647652837 - Validation Loss: 23885.706526131464\n",
      "Epoch 45/128 - Train Loss: 28570.877008563046 - Validation Loss: 23909.1083984375\n",
      "Epoch 46/128 - Train Loss: 28587.83152591089 - Validation Loss: 24360.143925107757\n",
      "Epoch 47/128 - Train Loss: 28584.195107805677 - Validation Loss: 24000.48471174569\n",
      "Epoch 48/128 - Train Loss: 28582.5229065741 - Validation Loss: 23800.708024649786\n",
      "Epoch 49/128 - Train Loss: 28576.745829353167 - Validation Loss: 24183.417598329743\n",
      "Epoch 50/128 - Train Loss: 28575.935073519377 - Validation Loss: 24222.21481007543\n",
      "Epoch 51/128 - Train Loss: 28691.11343477074 - Validation Loss: 23875.505084859913\n",
      "Epoch 52/128 - Train Loss: 28576.295491181085 - Validation Loss: 24171.246270541487\n",
      "Epoch 53/128 - Train Loss: 28581.349553936954 - Validation Loss: 23770.74778589709\n",
      "Epoch 54/128 - Train Loss: 28579.59753684498 - Validation Loss: 24291.633502828663\n",
      "Epoch 55/128 - Train Loss: 28578.952632454286 - Validation Loss: 23757.709540005388\n",
      "Epoch 56/128 - Train Loss: 28580.524311715337 - Validation Loss: 24311.181396484375\n",
      "Epoch 57/128 - Train Loss: 28593.549911299127 - Validation Loss: 23774.422110721982\n",
      "Epoch 58/128 - Train Loss: 28581.528232890967 - Validation Loss: 23733.523067079743\n",
      "Epoch 59/128 - Train Loss: 28608.329775347982 - Validation Loss: 23809.126700565732\n",
      "Epoch 60/128 - Train Loss: 28572.38668463428 - Validation Loss: 23834.191877693964\n",
      "Epoch 61/128 - Train Loss: 28580.25153947189 - Validation Loss: 23775.629899649786\n",
      "Epoch 62/128 - Train Loss: 28602.57953014124 - Validation Loss: 23924.794458849676\n",
      "Epoch 63/128 - Train Loss: 28580.61899989765 - Validation Loss: 24019.711408943964\n",
      "Epoch 64/128 - Train Loss: 28588.33409525109 - Validation Loss: 23755.75026939655\n",
      "Epoch 65/128 - Train Loss: 28603.403738230078 - Validation Loss: 23784.054940059268\n",
      "Epoch 66/128 - Train Loss: 28591.17201146288 - Validation Loss: 23949.656485721982\n",
      "Epoch 67/128 - Train Loss: 28578.09102074236 - Validation Loss: 23775.266913052263\n",
      "Epoch 68/128 - Train Loss: 28578.36076308338 - Validation Loss: 23805.53177195582\n",
      "Epoch 69/128 - Train Loss: 28573.00027719023 - Validation Loss: 23752.20834455819\n",
      "Epoch 70/128 - Train Loss: 28574.431361302537 - Validation Loss: 23780.753847319505\n",
      "Epoch 71/128 - Train Loss: 28578.168429312227 - Validation Loss: 23733.324942753232\n",
      "Epoch 72/128 - Train Loss: 28578.1673397414 - Validation Loss: 23776.785274110993\n",
      "Epoch 73/128 - Train Loss: 28687.316173836654 - Validation Loss: 23909.2666015625\n",
      "Epoch 74/128 - Train Loss: 28595.772034490994 - Validation Loss: 23795.135944234913\n",
      "Epoch 75/128 - Train Loss: 28573.589965287254 - Validation Loss: 24178.928702518857\n",
      "Epoch 76/128 - Train Loss: 28571.600095524016 - Validation Loss: 23877.529574690194\n",
      "Epoch 77/128 - Train Loss: 28583.12215346957 - Validation Loss: 23786.302599676725\n",
      "Epoch 78/128 - Train Loss: 28578.460696557722 - Validation Loss: 23742.799762594288\n",
      "Epoch 79/128 - Train Loss: 28586.415463376776 - Validation Loss: 23781.2282967403\n",
      "Epoch 80/128 - Train Loss: 28585.365306870906 - Validation Loss: 24100.207081761855\n",
      "Epoch 81/128 - Train Loss: 28576.767236967793 - Validation Loss: 24133.132349474676\n",
      "Epoch 82/128 - Train Loss: 28582.19994157683 - Validation Loss: 23783.125732421875\n",
      "Epoch 83/128 - Train Loss: 28582.775202988538 - Validation Loss: 24215.835331357757\n",
      "Epoch 84/128 - Train Loss: 28573.353741641648 - Validation Loss: 24072.725114493536\n",
      "Epoch 85/128 - Train Loss: 28592.825461841567 - Validation Loss: 23798.720989358837\n",
      "Epoch 86/128 - Train Loss: 28581.409992068096 - Validation Loss: 24005.324235587286\n",
      "Epoch 87/128 - Train Loss: 28585.27635865857 - Validation Loss: 23848.000555630388\n",
      "Epoch 88/128 - Train Loss: 28583.302344176445 - Validation Loss: 23858.130960398707\n",
      "Epoch 89/128 - Train Loss: 28592.77435435999 - Validation Loss: 23797.432415140087\n",
      "Epoch 90/128 - Train Loss: 28574.216338445007 - Validation Loss: 23809.727665342132\n",
      "Epoch 91/128 - Train Loss: 28616.38687653521 - Validation Loss: 23893.192450161638\n",
      "Epoch 92/128 - Train Loss: 28571.284736200192 - Validation Loss: 23829.084422144395\n",
      "Epoch 93/128 - Train Loss: 28582.07837660344 - Validation Loss: 23717.353204135237\n",
      "Epoch 94/128 - Train Loss: 28574.893273232807 - Validation Loss: 23721.342386179957\n",
      "Epoch 95/128 - Train Loss: 28581.2200805131 - Validation Loss: 23765.98903050916\n",
      "Epoch 96/128 - Train Loss: 28592.746703568504 - Validation Loss: 24168.213614628232\n",
      "Epoch 97/128 - Train Loss: 28611.65672975232 - Validation Loss: 24343.502794989225\n",
      "Epoch 98/128 - Train Loss: 28570.194922301445 - Validation Loss: 23779.827737742457\n",
      "Epoch 99/128 - Train Loss: 28576.08734477347 - Validation Loss: 24266.54248046875\n",
      "Epoch 100/128 - Train Loss: 28573.018603728848 - Validation Loss: 23839.7912345097\n",
      "Epoch 101/128 - Train Loss: 28570.889844602894 - Validation Loss: 24798.281670932112\n",
      "Epoch 102/128 - Train Loss: 28598.33713155022 - Validation Loss: 23819.557154162176\n",
      "Epoch 103/128 - Train Loss: 28578.72776934361 - Validation Loss: 23811.931236530174\n",
      "Epoch 104/128 - Train Loss: 28573.006172813184 - Validation Loss: 23944.21795864763\n",
      "Epoch 105/128 - Train Loss: 28577.853916484717 - Validation Loss: 23760.673053609913\n",
      "Epoch 106/128 - Train Loss: 28578.168838700873 - Validation Loss: 24063.318527747844\n",
      "Epoch 107/128 - Train Loss: 28584.238679977483 - Validation Loss: 24107.292573073813\n",
      "Epoch 108/128 - Train Loss: 28591.43841685999 - Validation Loss: 24213.051185344826\n",
      "Epoch 109/128 - Train Loss: 28610.63543088155 - Validation Loss: 23748.77260405442\n",
      "Epoch 110/128 - Train Loss: 28577.230961295714 - Validation Loss: 24092.977143386313\n",
      "Epoch 111/128 - Train Loss: 28580.10249855008 - Validation Loss: 23748.752988617995\n",
      "Epoch 112/128 - Train Loss: 28574.556265352076 - Validation Loss: 23833.549080684268\n",
      "Epoch 113/128 - Train Loss: 28577.551179977483 - Validation Loss: 24204.982708108837\n",
      "Epoch 114/128 - Train Loss: 28613.95433184361 - Validation Loss: 23947.519817483837\n",
      "Epoch 115/128 - Train Loss: 28583.29605835494 - Validation Loss: 23771.81982421875\n",
      "Epoch 116/128 - Train Loss: 28574.934557519104 - Validation Loss: 23746.23159684806\n",
      "Epoch 117/128 - Train Loss: 28578.81199466089 - Validation Loss: 23747.1599962958\n",
      "Epoch 118/128 - Train Loss: 28572.05523335153 - Validation Loss: 23817.668389682112\n",
      "Epoch 119/128 - Train Loss: 28584.341464246725 - Validation Loss: 23843.948107489225\n",
      "Epoch 120/128 - Train Loss: 28585.8422489083 - Validation Loss: 23753.57432819235\n",
      "Epoch 121/128 - Train Loss: 28573.63146492904 - Validation Loss: 23775.611092403018\n",
      "Epoch 122/128 - Train Loss: 28580.656412049673 - Validation Loss: 23882.89118904903\n",
      "Epoch 123/128 - Train Loss: 28572.74695090748 - Validation Loss: 24205.22157024515\n",
      "Epoch 124/128 - Train Loss: 28631.324542849346 - Validation Loss: 24096.017106681036\n",
      "Epoch 125/128 - Train Loss: 28585.66665742699 - Validation Loss: 23790.157580145475\n",
      "Epoch 126/128 - Train Loss: 28608.1565186613 - Validation Loss: 24399.218884698275\n",
      "Epoch 127/128 - Train Loss: 28587.445284780977 - Validation Loss: 23740.39321794181\n",
      "Epoch 128/128 - Train Loss: 28584.629443572598 - Validation Loss: 24459.49330717942\n",
      "Test Loss: 26599.042073567707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26599.042073567707"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, device, training_dataloader, validation_dataloader, test_dataloader, optimizer, loss_fn, 128, \"SecondCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SecondCount",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
