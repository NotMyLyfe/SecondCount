{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gordon/anaconda3/envs/SecondCount/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GraphNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/pdb/train_data.csv\")\n",
    "val_df = pd.read_csv(\"data/pdb/val_data.csv\")\n",
    "test_df = pd.read_csv(\"data/pdb/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondaryStructureDataset(Dataset):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVXWY\"\n",
    "    dssp_types = \"GHITEBSP-\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_one_hot(seq: str, charset: str) -> torch.Tensor:\n",
    "        one_hot = torch.zeros(len(seq), len(charset), dtype=torch.float)\n",
    "        for i, char in enumerate(seq):\n",
    "            if char in charset:\n",
    "                one_hot[i, charset.index(char)] = 1.0\n",
    "        return one_hot\n",
    "\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame,\n",
    "                 seq_col: str = \"sequence\",\n",
    "                 ss_col: str = \"secondary_structure\"):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.seq_col = seq_col\n",
    "        self.ss_col = ss_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        sequence = row[self.seq_col]\n",
    "        secondary_structure = row[self.ss_col]\n",
    "\n",
    "        x = self._to_one_hot(sequence, self.amino_acids)\n",
    "        y = self._to_one_hot(secondary_structure, self.dssp_types)\n",
    "\n",
    "        edge_idx = torch.tensor([[i, j] for i in range(len(sequence)) for j in range(len(sequence)) if i != j], dtype=torch.long).t().contiguous()\n",
    "        data = Data(x=x, edge_index=edge_idx, y=y)\n",
    "        return data\n",
    "    \n",
    "    def class_weights(self):\n",
    "        ss_cat = \"\".join(self.df[self.ss_col])\n",
    "        counts = {char: ss_cat.count(char) for char in self.dssp_types}\n",
    "        total = sum(counts.values())\n",
    "        weights = {char: total / count if count > 0 else 0 for char, count in counts.items()}\n",
    "        return torch.tensor([weights[char] for char in self.dssp_types], dtype=torch.float)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SecondaryStructureDataset(train_df)\n",
    "val_dataset = SecondaryStructureDataset(val_df)\n",
    "test_dataset = SecondaryStructureDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 8, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 8, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametric Exponential Linear Unit (PELU) activation function\n",
    "class PELU(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(PELU, self).__init__()\n",
    "        self.log_alpha = nn.Parameter(torch.log(torch.tensor(alpha)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        alpha = torch.exp(self.log_alpha)\n",
    "        return torch.where(x >= 0, x, alpha * (torch.exp(x) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels: int = len(SecondaryStructureDataset.amino_acids),\n",
    "                 out_channels: int = len(SecondaryStructureDataset.dssp_types),\n",
    "                 hidden_channels: int = 64,\n",
    "                 num_layers: int = 3):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        self.norms.append(GraphNorm(hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.norms.append(GraphNorm(hidden_channels))\n",
    "\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
    "        self.act = PELU()\n",
    "    \n",
    "    def forward(self, data: Data) -> torch.Tensor:\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for i, (conv, norm) in enumerate(zip(self.convs[:-1], self.norms)):\n",
    "            identity = x\n",
    "            x = conv(x, edge_index)\n",
    "            x = norm(x)\n",
    "            x = self.act(x)\n",
    "            x += identity\n",
    "\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(y_true, y_pred, name = \"\", tensorboard_writer = None, writer_val = 0):\n",
    "    target_names = list(SecondaryStructureDataset.dssp_types)\n",
    "    report = classification_report(y_true, y_pred, target_names=target_names, zero_division=0, output_dict=True)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    write_keys = target_names + [\"macro avg\", \"weighted avg\"]\n",
    "\n",
    "    if tensorboard_writer:\n",
    "        for key in write_keys:\n",
    "            if key in report:\n",
    "                tensorboard_writer.add_scalar(f\"Precision/{name}/{key}\", report[key][\"precision\"], writer_val)\n",
    "                tensorboard_writer.add_scalar(f\"Recall/{name}/{key}\", report[key][\"recall\"], writer_val)\n",
    "                tensorboard_writer.add_scalar(f\"F1/{name}/{key}\", report[key][\"f1-score\"], writer_val)\n",
    "        tensorboard_writer.add_scalar(f\"Accuracy/{name}\", acc, 0)\n",
    "\n",
    "    return report, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch: int,\n",
    "          model: GCN,\n",
    "          loader: SecondaryStructureDataset,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          batch_print_freq: int = 32,\n",
    "          writer: SummaryWriter = None) -> float:\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        running_loss += loss.item() * data.num_graphs\n",
    "\n",
    "        preds = out.argmax(dim=1)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(data.y.argmax(dim=1).cpu())\n",
    "\n",
    "        if writer is not None and len(all_preds) % batch_print_freq == batch_print_freq - 1:\n",
    "            avg_loss = running_loss / (batch_print_freq * data.num_graphs)\n",
    "            writer.add_scalar(\"Loss/train\", avg_loss, epoch * len(loader) + len(all_preds))\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Write F1 score, recall, precision, and accuracy\n",
    "            last_few_preds = torch.cat(all_preds[-batch_print_freq:]).numpy()\n",
    "            last_few_labels = torch.cat(all_labels[-batch_print_freq:]).numpy()\n",
    "\n",
    "            classification_metrics(\n",
    "                last_few_labels,\n",
    "                last_few_preds,\n",
    "                name=\"train/batch\",\n",
    "                tensorboard_writer=writer,\n",
    "                writer_val=epoch * len(loader) + len(all_preds)\n",
    "            )\n",
    "\n",
    "    total_loss /= len(loader.dataset)\n",
    "\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    report,accuracy = classification_metrics(\n",
    "        all_labels,\n",
    "        all_preds,\n",
    "        name=\"train/epoch\",\n",
    "        tensorboard_writer=writer,\n",
    "        writer_val=epoch\n",
    "    )\n",
    "\n",
    "    f1, recall, precision = report[\"macro avg\"][\"f1-score\"], report[\"macro avg\"][\"recall\"], report[\"macro avg\"][\"precision\"]\n",
    "\n",
    "    return total_loss, f1, recall, precision, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: GCN,\n",
    "            loader: SecondaryStructureDataset,\n",
    "            criterion: nn.Module,\n",
    "            dataset_name: str,\n",
    "            epoch: int = 0,\n",
    "            writer: SummaryWriter = None) -> float:\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(data.y.argmax(dim=1).cpu())\n",
    "\n",
    "    total_loss /= len(loader.dataset)\n",
    "\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    \n",
    "    report, accuracy = classification_metrics(\n",
    "        all_labels,\n",
    "        all_preds,\n",
    "        name=f\"{dataset_name}/epoch\",\n",
    "        tensorboard_writer=writer,\n",
    "        writer_val=epoch\n",
    "    )\n",
    "\n",
    "    f1, recall, precision = report[\"macro avg\"][\"f1-score\"], report[\"macro avg\"][\"recall\"], report[\"macro avg\"][\"precision\"]\n",
    "\n",
    "    return total_loss, f1, recall, precision, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: GCN,\n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                test_loader: DataLoader,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                criterion: nn.Module,\n",
    "                epochs: int = 100,\n",
    "                best_val_loss: float = float(\"inf\"),\n",
    "                batch_print_freq: int = 32,\n",
    "                model_name: str = None,) -> GCN:\n",
    "    if model_name is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        model_name = f\"SecondCount-{timestamp}\"\n",
    "\n",
    "    writer = SummaryWriter(f\"runs/{model_name}\")\n",
    "\n",
    "    model.to(device)\n",
    "    pathlib.Path(f\"models/{model_name}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        train_loss, _, _, _, _ = train(\n",
    "            epoch, model, train_loader, optimizer, criterion, batch_print_freq, writer\n",
    "        )\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        val_loss, val_f1, val_recall, val_precision, val_accuracy = evaluate(\n",
    "            model, val_loader, criterion, \"val\", epoch, writer\n",
    "        )\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, F1: {val_f1:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        save_state = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "        }\n",
    "\n",
    "        torch.save(save_state, f\"models/{model_name}/checkpoint.pth\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(save_state, f\"models/{model_name}/best_model.pth\")\n",
    "            print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete. Evaluating on test set...\")\n",
    "    test_loss, test_f1, test_recall, test_precision, test_accuracy = evaluate(\n",
    "        model, test_loader, criterion, \"test\", writer\n",
    "    )\n",
    "    print(f\"Test Loss: {test_loss:.4f}, F1: {test_f1:.4f}, Recall: {test_recall:.4f}, Precision: {test_precision:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
    "    writer.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(weight=train_dataset.class_weights().to(device))\n",
    "trained_model = train_model(\n",
    "    model,\n",
    "    training_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SecondCount",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
