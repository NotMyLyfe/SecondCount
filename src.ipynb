{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "distributed = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    distributed = torch.cuda.device_count() > 1\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GraphNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondaryStructureDataset(Dataset):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVXWY\"\n",
    "    dssp_types = \"GHITEBSP-\"\n",
    "    edge_distances = [1, 2, 3, 4, 16, 32, 64, 128]\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_one_hot(seq: str, charset: str) -> torch.Tensor:\n",
    "        one_hot = torch.zeros(len(seq), len(charset), dtype=torch.float)\n",
    "        for i, char in enumerate(seq):\n",
    "            if char in charset:\n",
    "                one_hot[i, charset.index(char)] = 1.0\n",
    "        return one_hot\n",
    "\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame,\n",
    "                 seq_col: str = \"sequence\",\n",
    "                 ss_col: str = \"secondary_structure\"):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.seq_col = seq_col\n",
    "        self.ss_col = ss_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        sequence = row[self.seq_col]\n",
    "        secondary_structure = row[self.ss_col]\n",
    "\n",
    "        x = self._to_one_hot(sequence, self.amino_acids)\n",
    "        y = self._to_one_hot(secondary_structure, self.dssp_types)\n",
    "\n",
    "        edge_idx = []\n",
    "        for i in range(len(sequence)):\n",
    "            for j in SecondaryStructureDataset.edge_distances:\n",
    "                if i + j < len(sequence):\n",
    "                    edge_idx.append((i, i + j))\n",
    "                if i - j >= 0:\n",
    "                    edge_idx.append((i, i - j))\n",
    "\n",
    "        edge_idx = torch.tensor(edge_idx, dtype=torch.long).t().contiguous()\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_idx, y=y)\n",
    "        return data\n",
    "    \n",
    "    def class_weights(self):\n",
    "        ss_cat = \"\".join(self.df[self.ss_col])\n",
    "        counts = {char: ss_cat.count(char) for char in self.dssp_types}\n",
    "        total = sum(counts.values())\n",
    "        weights = {char: total / count if count > 0 else 0 for char, count in counts.items()}\n",
    "        return torch.tensor([weights[char] for char in self.dssp_types], dtype=torch.float)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametric Exponential Linear Unit (PELU) activation function\n",
    "class PELU(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(PELU, self).__init__()\n",
    "        self.log_alpha = nn.Parameter(torch.log(torch.tensor(alpha)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        alpha = torch.exp(self.log_alpha)\n",
    "        return torch.where(x >= 0, x, alpha * (torch.exp(x) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = len(SecondaryStructureDataset.amino_acids),\n",
    "                 out_channels: int = len(SecondaryStructureDataset.dssp_types),\n",
    "                 hidden_channels: int = 64,\n",
    "                 num_layers: int = 3):\n",
    "        super(GCN, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        self.norms.append(GraphNorm(hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.norms.append(GraphNorm(hidden_channels))\n",
    "\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
    "        self.act = PELU()\n",
    "\n",
    "    def forward(self, data: Data) -> torch.Tensor:\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        for i, (conv, norm) in enumerate(zip(self.convs[:-1], self.norms)):\n",
    "            identity = x\n",
    "            x = conv(x, edge_index)\n",
    "            x = norm(x)\n",
    "            x = self.act(x)\n",
    "            if i != 0:\n",
    "              x = x + identity\n",
    "\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics import MetricCollection, Accuracy, F1Score, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch: int,\n",
    "          model: GCN,\n",
    "          loader: SecondaryStructureDataset,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "          criterion: nn.Module,\n",
    "          device: str | torch.device | int,\n",
    "          batch_print_freq: int = 32,\n",
    "          writer: SummaryWriter = None,\n",
    "          distributed: bool = False) -> float:\n",
    "    model.train()\n",
    "\n",
    "    total_loss = torch.zeros(1, device=device, requires_grad=False)\n",
    "    running_loss = torch.zeros(1, device=device, requires_grad=False)\n",
    "    total_count = torch.zeros(1, device=device, requires_grad=False)\n",
    "    running_count = torch.zeros(1, device=device, requires_grad=False)\n",
    "\n",
    "    num_class = len(SecondaryStructureDataset.dssp_types)\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    for avg in [None, 'macro', 'micro', 'weighted']:\n",
    "        metrics[f'accuracy_{avg}'] = Accuracy(num_classes=num_class, task='multiclass', average=avg)\n",
    "        metrics[f'f1_score_{avg}'] = F1Score(num_classes=num_class, task='multiclass', average=avg)\n",
    "        metrics[f'precision_{avg}'] = Precision(num_classes=num_class, task='multiclass', average=avg)\n",
    "        metrics[f'recall_{avg}'] = Recall(num_classes=num_class, task='multiclass', average=avg)\n",
    "\n",
    "    distTorchMetricsBatch = MetricCollection(metrics=metrics).to(device)\n",
    "\n",
    "    distTorchMetricsEpoch = MetricCollection(metrics=metrics).to(device)\n",
    "\n",
    "    for i, data in enumerate(loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        running_loss += loss.item() * data.num_graphs\n",
    "        total_count += data.num_graphs\n",
    "        running_count += data.num_graphs\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        distTorchMetricsBatch.update(out.argmax(dim=1), data.y.argmax(dim=1))\n",
    "        distTorchMetricsEpoch.update(out.argmax(dim=1), data.y.argmax(dim=1))\n",
    "\n",
    "        if i % batch_print_freq == 0 and (writer is not None or distributed):\n",
    "            if distributed:\n",
    "                torch.distributed.all_reduce(\n",
    "                    running_loss,\n",
    "                    op=torch.distributed.ReduceOp.SUM\n",
    "                )\n",
    "                torch.distributed.all_reduce(\n",
    "                    running_count,\n",
    "                    op=torch.distributed.ReduceOp.SUM\n",
    "                )\n",
    "\n",
    "            if writer is not None:\n",
    "                local_stats = distTorchMetricsBatch.compute()\n",
    "                for key, value in local_stats.items():\n",
    "                    if distributed:\n",
    "                        torch.distributed.all_reduce(value, op=torch.distributed.ReduceOp.SUM)\n",
    "                        value /= torch.distributed.get_world_size()\n",
    "                    if value.dim() > 0 and value.size(0) > 1:\n",
    "                        for idx, cls in enumerate(SecondaryStructureDataset.dssp_types):\n",
    "                            writer.add_scalar(f\"{key}/train/batch/{cls}\", value[idx].item(), epoch * len(loader) + i)\n",
    "                    else:\n",
    "                        writer.add_scalar(f\"{key}/train/batch\", value.item(), epoch * len(loader) + i)\n",
    "\n",
    "                writer.add_scalar(\"Loss/train/batch\", running_loss.item() / running_count.item(), epoch * len(loader) + i)\n",
    "                distTorchMetricsBatch.reset()\n",
    "\n",
    "            running_loss.zero_()\n",
    "            running_count.zero_()\n",
    "\n",
    "    if distributed:\n",
    "        torch.distributed.all_reduce(total_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        torch.distributed.all_reduce(total_count, op=torch.distributed.ReduceOp.SUM)\n",
    "    total_loss /= total_count.item()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_scalar(\"Loss/train/epoch\", total_loss.item(), epoch)\n",
    "    \n",
    "    epoch_stats = distTorchMetricsEpoch.compute()\n",
    "    for key, value in epoch_stats.items():\n",
    "        if distributed:\n",
    "            torch.distributed.all_reduce(value, op=torch.distributed.ReduceOp.SUM)\n",
    "            value /= torch.distributed.get_world_size()\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(f\"{key}/train/epoch\", value.item(), epoch)\n",
    "            for idx, cls in enumerate(SecondaryStructureDataset.dssp_types):\n",
    "                writer.add_scalar(f\"{key}/train/epoch/{cls}\", value[idx].item(), epoch)\n",
    "\n",
    "    return total_loss.item(), epoch_stats['accuracy'].item(), epoch_stats['f1_score'].item(), epoch_stats['precision'].item(), epoch_stats['recall'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: GCN,\n",
    "            loader: DataLoader,\n",
    "            criterion: nn.Module,\n",
    "            dataset_name: str,\n",
    "            device: str | torch.device | int,\n",
    "            epoch: int = 0,\n",
    "            writer: SummaryWriter = None,\n",
    "            distributed: bool = False) -> float:\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = torch.zeros(1, device=device, requires_grad=False)\n",
    "    total_count = torch.zeros(1, device=device, requires_grad=False)\n",
    "    \n",
    "    num_class = len(SecondaryStructureDataset.dssp_types)\n",
    "\n",
    "    metricDict = {}\n",
    "\n",
    "    for avg in [None, 'macro', 'micro', 'weighted']:\n",
    "        metricDict[f'accuracy_{avg}'] = Accuracy(num_classes=num_class, task='multiclass', average=avg)\n",
    "        metricDict[f'f1_score_{avg}'] = F1Score(num_classes=num_class, task='multiclass', average=avg)\n",
    "        metricDict[f'precision_{avg}'] = Precision(num_classes=num_class, task='multiclass', average=avg)\n",
    "        metricDict[f'recall_{avg}'] = Recall(num_classes=num_class, task='multiclass', average=avg)\n",
    "\n",
    "    metrics = MetricCollection(metricDict).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            total_count += data.num_graphs\n",
    "            \n",
    "            metrics.update(out.argmax(dim=1), data.y.argmax(dim=1))\n",
    "\n",
    "    if distributed:\n",
    "        torch.distributed.all_reduce(total_loss, op=torch.distributed.ReduceOp.SUM)\n",
    "        torch.distributed.all_reduce(total_count, op=torch.distributed.ReduceOp.SUM)\n",
    "    \n",
    "    total_loss /= total_count.item()\n",
    "    \n",
    "    # Compute metrics\n",
    "    stats = metrics.compute()\n",
    "    accuracy = stats['accuracy']\n",
    "    f1 = stats['f1_score']\n",
    "    recall = stats['recall']\n",
    "    precision = stats['precision']\n",
    "    \n",
    "    # Handle distributed metrics\n",
    "    if distributed:\n",
    "        for key, value in stats.items():\n",
    "            torch.distributed.all_reduce(value, op=torch.distributed.ReduceOp.SUM)\n",
    "            value /= torch.distributed.get_world_size()\n",
    "    \n",
    "    # Write to tensorboard\n",
    "    if writer is not None:\n",
    "        writer.add_scalar(f\"Loss/{dataset_name}/epoch\", total_loss.item(), epoch)\n",
    "        for key, value in stats.items():\n",
    "            if value.dim() > 0 and value.size(0) > 1:\n",
    "                for idx, cls in enumerate(SecondaryStructureDataset.dssp_types):\n",
    "                    writer.add_scalar(f\"{key}/{dataset_name}/epoch/{cls}\", stats[key][idx].item(), epoch)\n",
    "            else:\n",
    "                writer.add_scalar(f\"{key}/{dataset_name}/epoch\", value.item(), epoch)\n",
    "    \n",
    "    return total_loss.item(), f1.item(), recall.item(), precision.item(), accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddp_setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    torch.cuda.set_device(rank)\n",
    "    init_process_group(\"gloo\", rank=rank, world_size=world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(save_every: int,\n",
    "        total_epochs: int,\n",
    "        train_dataloader: DataLoader,\n",
    "        val_dataloader: DataLoader,\n",
    "        test_dataloader: DataLoader,\n",
    "        device: str | torch.device | int,\n",
    "        distributed: bool = False):\n",
    "    model = GCN().to(device)\n",
    "    model_name = f\"SecondCount-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "    if distributed:\n",
    "        model = DDP(model, device_ids=[device])\n",
    "\n",
    "    writer = SummaryWriter(f\"runs/{model_name}\") if not distributed or torch.distributed.get_rank() == 0 else None\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(weight=train_dataloader.dataset.class_weights().to(device))\n",
    "\n",
    "    steps_per_epoch = len(train_dataloader)\n",
    "\n",
    "    warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer=optimizer,\n",
    "        start_factor=0.1,\n",
    "        total_iters=steps_per_epoch // 5\n",
    "    )\n",
    "\n",
    "    cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer=optimizer,\n",
    "        T_0=steps_per_epoch * 25\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ChainedScheduler(\n",
    "        schedulers=[warmup, cosine]\n",
    "    )\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{total_epochs}\")\n",
    "        \n",
    "        train_loss, train_accuracy, train_f1, train_precision, train_recall = train(\n",
    "            epoch, model, train_dataloader, optimizer, criterion, device,\n",
    "            batch_print_freq=save_every, writer=writer, distributed=distributed\n",
    "        )\n",
    "        if not distributed or torch.distributed.get_rank() == 0:\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, F1: {train_f1:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        val_loss, val_f1, val_recall, val_precision, val_accuracy = evaluate(\n",
    "            model, val_dataloader, criterion, \"val\", device, epoch, writer, distributed, \n",
    "        )\n",
    "        if not distributed or torch.distributed.get_rank() == 0:\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, F1: {val_f1:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"models/{model_name}/best_model.pth\")\n",
    "            print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "        save_state = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "        }\n",
    "\n",
    "        torch.save(save_state, f\"models/{model_name}/checkpoint-{epoch}{f\"-{device}\" if distributed else \"\"}.pth\")\n",
    "\n",
    "    if not distributed or torch.distributed.get_rank() == 0:\n",
    "        print(\"Training complete. Evaluating on test set...\")\n",
    "    test_loss, test_f1, test_recall, test_precision, test_accuracy = evaluate(\n",
    "        model, test_dataloader, criterion, \"test\", device, writer=writer, distributed=distributed\n",
    "    )\n",
    "\n",
    "    if not distributed or torch.distributed.get_rank() == 0:\n",
    "        print(f\"Test Loss: {test_loss:.4f}, F1: {test_f1:.4f}, Recall: {test_recall:.4f}, Precision: {test_precision:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
    "        writer.close()\n",
    "    \n",
    "    if distributed:\n",
    "        destroy_process_group()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_run(rank: int, \n",
    "             world_size: int, \n",
    "             save_every: int,\n",
    "             total_epochs: int, \n",
    "             batch_size: int,\n",
    "             train_dataset: SecondaryStructureDataset,\n",
    "             val_dataset: SecondaryStructureDataset,\n",
    "             test_dataset: SecondaryStructureDataset):\n",
    "    ddp_setup(rank, world_size)\n",
    "    \n",
    "    torch.manual_seed(432250798121)\n",
    "\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank)\n",
    "    test_sampler = DistributedSampler(test_dataset, num_replicas=world_size, rank=rank)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "    run(\n",
    "        save_every=save_every,\n",
    "        total_epochs=total_epochs,\n",
    "        batch_size=batch_size,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        test_dataloader=test_loader,\n",
    "        device=rank,\n",
    "        distributed=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_df = pd.read_csv(\"data/pdb/train_data.csv\")\n",
    "    val_df = pd.read_csv(\"data/pdb/val_data.csv\")\n",
    "    test_df = pd.read_csv(\"data/pdb/test_data.csv\")\n",
    "\n",
    "    train_dataset = SecondaryStructureDataset(train_df)\n",
    "    val_dataset = SecondaryStructureDataset(val_df)\n",
    "    test_dataset = SecondaryStructureDataset(test_df)\n",
    "\n",
    "    training_loader = DataLoader(train_dataset, batch_size = 8, shuffle = True, sampler=DistributedSampler(train_dataset) if distributed else None)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = 8, shuffle = True, sampler=DistributedSampler(val_dataset) if distributed else None)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = 8, shuffle = True, sampler=DistributedSampler(test_dataset) if distributed else None)\n",
    "    if distributed:\n",
    "        world_size = torch.cuda.device_count()\n",
    "        mp.spawn(dist_run, args=(world_size, 32, 10, 8, train_dataset, val_dataset, test_dataset), nprocs=world_size, join=True)\n",
    "    else:\n",
    "        run(\n",
    "            save_every=32,\n",
    "            total_epochs=10,\n",
    "            train_dataloader=training_loader,\n",
    "            val_dataloader=val_loader,\n",
    "            test_dataloader=test_loader,\n",
    "            device=device,\n",
    "            distributed=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SecondCount",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
