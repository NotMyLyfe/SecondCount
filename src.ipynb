{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rcsb_data = pd.read_csv(\"data/rcsb/RCSB_PDB_Macromolecular_Structure_Dataset_with_Structural_Features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = rcsb_data.loc[:,[\"Sequence\", \"Number of Residues\", \"Molecular Weight per Deposited Model\", \"Molecular Weight (Entity)\", \"R Free\", \"R Work\", \"Helix\", \"Sheet\", \"Coil\"]]\n",
    "filtered_data = filtered_data.dropna(subset=[\"Helix\", \"Sheet\", \"Coil\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amino_acid_tokenizer(amino_acid : str) -> torch.Tensor:\n",
    "    amino_acid_tokens = {\n",
    "        \"A\": 1,\n",
    "        \"C\": 2,\n",
    "        \"D\": 3,\n",
    "        \"E\": 4,\n",
    "        \"F\": 5,\n",
    "        \"G\": 6,\n",
    "        \"H\": 7,\n",
    "        \"I\": 8,\n",
    "        \"K\": 9,\n",
    "        \"L\": 10,\n",
    "        \"M\": 11,\n",
    "        \"N\": 12,\n",
    "        \"P\": 13,\n",
    "        \"Q\": 14,\n",
    "        \"R\": 15,\n",
    "        \"S\": 16,\n",
    "        \"T\": 17,\n",
    "        \"V\": 18,\n",
    "        \"W\": 19,\n",
    "        \"Y\": 20,\n",
    "        \"X\": 21,\n",
    "        \"U\": 22,\n",
    "        \"O\": 23\n",
    "    }\n",
    "    return torch.tensor([amino_acid_tokens[aa] for aa in amino_acid], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data[\"Sequence\"] = filtered_data[\"Sequence\"].apply(amino_acid_tokenizer)\n",
    "\n",
    "# Padding sequences with zeros to make them all the same length\n",
    "max_sequence_length = filtered_data[\"Sequence\"].apply(len).max()\n",
    "filtered_data[\"Sequence\"] = filtered_data[\"Sequence\"].apply(lambda x: torch.nn.functional.pad(x, (0, max_sequence_length - len(x)), \"constant\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrambled_data = filtered_data.sample(frac=1)\n",
    "scrambled_data = scrambled_data.reset_index(drop=True)\n",
    "\n",
    "data_size = len(scrambled_data)\n",
    "train_size = int(data_size * 0.8)\n",
    "test_size = data_size - train_size\n",
    "validation_size = int(train_size * 0.2)\n",
    "\n",
    "train_data = scrambled_data.iloc[:train_size - validation_size]\n",
    "validation_data = scrambled_data.iloc[train_size - validation_size:train_size]\n",
    "test_data = scrambled_data.iloc[train_size:]\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "validation_data = validation_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_df = train_data.loc[:,[\"Sequence\", \"Number of Residues\", \"Molecular Weight per Deposited Model\", \"Molecular Weight (Entity)\", \"R Free\", \"R Work\"]]\n",
    "train_output_df = train_data.loc[:,[\"Helix\", \"Sheet\", \"Coil\"]]\n",
    "validation_input_df = validation_data.loc[:,[\"Sequence\", \"Number of Residues\", \"Molecular Weight per Deposited Model\", \"Molecular Weight (Entity)\", \"R Free\", \"R Work\"]]\n",
    "validation_output_df = validation_data.loc[:,[\"Helix\", \"Sheet\", \"Coil\"]]\n",
    "test_input_df = test_data.loc[:,[\"Sequence\", \"Number of Residues\", \"Molecular Weight per Deposited Model\", \"Molecular Weight (Entity)\", \"R Free\", \"R Work\"]]\n",
    "test_output_df = test_data.loc[:,[\"Helix\", \"Sheet\", \"Coil\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(input_df, output_df):\n",
    "    input_tensors = torch.tensor(input_df.drop(columns=[\"Sequence\"]).values, dtype=torch.float32)\n",
    "    input_tensor_sequences = torch.stack(tuple(input_df[\"Sequence\"].values), 0)\n",
    "\n",
    "    output_tensors = torch.tensor(output_df.values, dtype=torch.float32)\n",
    "\n",
    "    return torch.utils.data.TensorDataset(input_tensors, input_tensor_sequences, output_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_input_df, train_output_df)\n",
    "validation_dataset = create_dataset(validation_input_df, validation_output_df)\n",
    "test_dataset = create_dataset(test_input_df, test_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch : int,\n",
    "          model : nn.Module,\n",
    "          device: torch.device,\n",
    "          train_dataloader : DataLoader,\n",
    "          optimizer : Optimizer,\n",
    "          loss_fn : nn.Module,\n",
    "          tensorboard_writer : SummaryWriter = None) -> float:\n",
    "    running_loss = 0.\n",
    "\n",
    "    model.train(True)\n",
    "\n",
    "    for idx, (input_tensors, input_tensor_sequences, output_tensors) in enumerate(train_dataloader):\n",
    "        input_tensors = input_tensors.to(device)\n",
    "        input_tensor_sequences = input_tensor_sequences.to(device)\n",
    "        output_tensors = output_tensors.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(input_tensors, input_tensor_sequences)\n",
    "        loss = loss_fn(output, output_tensors)\n",
    "  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "  \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if tensorboard_writer is not None:\n",
    "            tensorboard_writer.add_scalar(\"Loss/train\", loss.item(), epoch * len(train_dataloader) + idx)\n",
    "\n",
    "    running_loss /= len(train_dataloader)\n",
    "\n",
    "    if tensorboard_writer is not None:\n",
    "        tensorboard_writer.add_scalar(\"Loss/train/epoch\", running_loss, epoch)\n",
    "    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch : int, \n",
    "             model : nn.Module,\n",
    "             device : torch.device,\n",
    "             validation_dataloader : DataLoader,\n",
    "             loss_fn : nn.Module,\n",
    "             tensorboard_writer : SummaryWriter = None) -> float:\n",
    "    running_loss = 0.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (input_tensors, input_tensor_sequences, output_tensors) in enumerate(validation_dataloader):\n",
    "            input_tensors = input_tensors.to(device)\n",
    "            input_tensor_sequences = input_tensor_sequences.to(device)\n",
    "            output_tensors = output_tensors.to(device)\n",
    "\n",
    "            output = model(input_tensors, input_tensor_sequences)\n",
    "            loss = loss_fn(output, output_tensors)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if tensorboard_writer is not None:\n",
    "                tensorboard_writer.add_scalar(\"Loss/validation\", loss.item(), epoch * len(validation_dataloader) + idx)\n",
    "    \n",
    "    running_loss /= len(validation_dataloader)\n",
    "\n",
    "    if tensorboard_writer is not None:\n",
    "        tensorboard_writer.add_scalar(\"Loss/validation/epoch\", running_loss, epoch)\n",
    "    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model : nn.Module,\n",
    "         device : torch.device,\n",
    "         test_dataloader : DataLoader,\n",
    "         loss_fn : nn.Module) -> float:\n",
    "    running_loss = 0.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (input_tensors, input_tensor_sequences, output_tensors) in enumerate(test_dataloader):\n",
    "            input_tensors = input_tensors.to(device)\n",
    "            input_tensor_sequences = input_tensor_sequences.to(device)\n",
    "            output_tensors = output_tensors.to(device)\n",
    "\n",
    "            output = model(input_tensors, input_tensor_sequences)\n",
    "            loss = loss_fn(output, output_tensors)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    running_loss /= len(test_dataloader)\n",
    "\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model : nn.Module,\n",
    "                device : torch.device,\n",
    "                training_dataloader : DataLoader,\n",
    "                validation_dataloader : DataLoader,\n",
    "                test_dataloader : DataLoader,\n",
    "                optimizer : Optimizer,\n",
    "                loss_fn : nn.Module,\n",
    "                epochs : int,\n",
    "                model_name : str = \"SecondCount\") -> float:\n",
    "    model.to(device)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    tensorboard_writer = SummaryWriter(f\"runs/{model_name}_{timestamp}\")\n",
    "\n",
    "    best_validation_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(epoch, model, device, training_dataloader, optimizer, loss_fn, tensorboard_writer)\n",
    "        validation_loss = validate(epoch, model, device, validation_dataloader, loss_fn, tensorboard_writer)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss} - Validation Loss: {validation_loss}\")\n",
    "\n",
    "        if validation_loss < best_validation_loss:\n",
    "            best_validation_loss = validation_loss\n",
    "            torch.save(model.state_dict(), f\"models/{model_name}_{timestamp}_{epoch}.pt\")\n",
    "\n",
    "\n",
    "    test_loss = test(model, device, test_dataloader, loss_fn)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "    tensorboard_writer.close()\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondCountModel(nn.Module):\n",
    "    def __init__(self, max_sequence_length : int, structural_features : int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.structural_features = structural_features\n",
    "\n",
    "        # Encoder-Decoder LSTM Model\n",
    "\n",
    "        self.encoder_lstm = nn.LSTM(input_size=max_sequence_length, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.decoder_lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=1, batch_first=True)\n",
    "\n",
    "        self.encoder_decoder = nn.Sequential(\n",
    "            self.encoder_lstm,\n",
    "            self.decoder_lstm\n",
    "        )\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 + structural_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_tensors, input_tensor_sequences):\n",
    "        # Sequence Encoding/Decoding using self.encoder_decoder\n",
    "        encoded_decoded, _ = self.encoder_decoder(input_tensor_sequences)\n",
    "\n",
    "        # Applying values from input_tensors and encoded_decoded to the fully connected layer\n",
    "        concatenated = torch.cat((input_tensors, encoded_decoded), 1)\n",
    "        return self.fc(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SecondCountModel(max_sequence_length, train_dataset.tensors[0].shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "input must have the type torch.float32, got type torch.int32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(model, device, training_dataloader, validation_dataloader, test_dataloader, optimizer, loss_fn, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSecondCount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, device, training_dataloader, validation_dataloader, test_dataloader, optimizer, loss_fn, epochs, model_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m best_validation_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 18\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(epoch, model, device, training_dataloader, optimizer, loss_fn, tensorboard_writer)\n\u001b[1;32m     19\u001b[0m     validation_loss \u001b[38;5;241m=\u001b[39m validate(epoch, model, device, validation_dataloader, loss_fn, tensorboard_writer)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, model, device, train_dataloader, optimizer, loss_fn, tensorboard_writer)\u001b[0m\n\u001b[1;32m     15\u001b[0m output_tensors \u001b[38;5;241m=\u001b[39m output_tensors\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_tensors, input_tensor_sequences)\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, output_tensors)\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/SecondCount/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/SecondCount/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 30\u001b[0m, in \u001b[0;36mSecondCountModel.forward\u001b[0;34m(self, input_tensors, input_tensor_sequences)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_tensors, input_tensor_sequences):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Sequence Encoding/Decoding using self.encoder_decoder\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     encoded_decoded, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_decoder(input_tensor_sequences)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Applying values from input_tensors and encoded_decoded to the fully connected layer\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     concatenated \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((input_tensors, encoded_decoded), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/SecondCount/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/SecondCount/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/SecondCount/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/SecondCount/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/SecondCount/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/SecondCount/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1100\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m   1093\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[1;32m   1094\u001b[0m         max_batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m   1098\u001b[0m     )\n\u001b[1;32m   1099\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n\u001b[0;32m-> 1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "File \u001b[0;32m~/anaconda3/envs/SecondCount/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1000\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    997\u001b[0m     hidden: Tuple[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    998\u001b[0m     batch_sizes: Optional[Tensor],\n\u001b[1;32m    999\u001b[0m ):\n\u001b[0;32m-> 1000\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[1;32m   1002\u001b[0m         hidden[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1005\u001b[0m     )\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[1;32m   1007\u001b[0m         hidden[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   1008\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m   1009\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1010\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/SecondCount/lib/python3.12/site-packages/torch/nn/modules/rnn.py:303\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_is_any_autocast_enabled()\n\u001b[1;32m    302\u001b[0m     ):\n\u001b[0;32m--> 303\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    304\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must have the type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         )\n\u001b[1;32m    306\u001b[0m expected_input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m expected_input_dim:\n",
      "\u001b[0;31mValueError\u001b[0m: input must have the type torch.float32, got type torch.int32"
     ]
    }
   ],
   "source": [
    "train_model(model, device, training_dataloader, validation_dataloader, test_dataloader, optimizer, loss_fn, 10, \"SecondCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SecondCount",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
